{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb07e6f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2b5ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from fractions import gcd\n",
    "from numbers import Number\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from numpy import float64, ndarray\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "from fractions import gcd\n",
    "from numbers import Number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c01a059",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee1b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "config = dict()\n",
    "\"\"\"Train\"\"\"\n",
    "config[\"display_iters\"] = 205942\n",
    "config[\"val_iters\"] = 205942 * 2\n",
    "config[\"save_freq\"] = 1.0\n",
    "config[\"epoch\"] = 0\n",
    "config[\"horovod\"] = True\n",
    "config[\"opt\"] = \"adam\"\n",
    "config[\"num_epochs\"] = 36\n",
    "config[\"lr\"] = [1e-3, 1e-4]\n",
    "config[\"lr_epochs\"] = [32]\n",
    "#config[\"lr_func\"] = StepLR(config[\"lr\"], config[\"lr_epochs\"])\n",
    "\n",
    "config[\"batch_size\"] = 32\n",
    "config[\"val_batch_size\"] = 32\n",
    "config[\"workers\"] = 0\n",
    "config[\"val_workers\"] = config[\"workers\"]\n",
    "\n",
    "\"\"\"Model\"\"\"\n",
    "config[\"rot_aug\"] = False\n",
    "config[\"pred_range\"] = [-100.0, 100.0, -100.0, 100.0]\n",
    "config[\"num_scales\"] = 6\n",
    "config[\"n_actor\"] = 128\n",
    "config[\"n_map\"] = 128\n",
    "config[\"actor2map_dist\"] = 7.0\n",
    "config[\"map2actor_dist\"] = 6.0\n",
    "config[\"actor2actor_dist\"] = 100.0\n",
    "config[\"pred_size\"] = 30\n",
    "config[\"pred_step\"] = 1\n",
    "config[\"num_preds\"] = config[\"pred_size\"] // config[\"pred_step\"]\n",
    "config[\"num_mods\"] = 6\n",
    "config[\"cls_coef\"] = 1.0\n",
    "config[\"reg_coef\"] = 1.0\n",
    "config[\"mgn\"] = 0.2\n",
    "config[\"cls_th\"] = 2.0\n",
    "config[\"cls_ignore\"] = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b195397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_in, n_out, norm='GN', ng=32, act=True):\n",
    "        super(Linear, self).__init__()\n",
    "        assert(norm in ['GN', 'BN', 'SyncBN'])\n",
    "\n",
    "        self.linear = nn.Linear(n_in, n_out, bias=False)\n",
    "        \n",
    "        if norm == 'GN':\n",
    "            self.norm = nn.GroupNorm(gcd(ng, n_out), n_out)\n",
    "        elif norm == 'BN':\n",
    "            self.norm = nn.BatchNorm1d(n_out)\n",
    "        else:\n",
    "            exit('SyncBN has not been added!')\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = self.norm(out)\n",
    "        if self.act:\n",
    "            out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class LinearRes(nn.Module):\n",
    "    def __init__(self, n_in, n_out, norm='GN', ng=32):\n",
    "        super(LinearRes, self).__init__()\n",
    "        assert(norm in ['GN', 'BN', 'SyncBN'])\n",
    "\n",
    "        self.linear1 = nn.Linear(n_in, n_out, bias=False)\n",
    "        self.linear2 = nn.Linear(n_out, n_out, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if norm == 'GN':\n",
    "            self.norm1 = nn.GroupNorm(gcd(ng, n_out), n_out)\n",
    "            self.norm2 = nn.GroupNorm(gcd(ng, n_out), n_out)\n",
    "        elif norm == 'BN':\n",
    "            self.norm1 = nn.BatchNorm1d(n_out)\n",
    "            self.norm2 = nn.BatchNorm1d(n_out)\n",
    "        else:   \n",
    "            exit('SyncBN has not been added!')\n",
    "\n",
    "        if n_in != n_out:\n",
    "            if norm == 'GN':\n",
    "                self.transform = nn.Sequential(\n",
    "                    nn.Linear(n_in, n_out, bias=False),\n",
    "                    nn.GroupNorm(gcd(ng, n_out), n_out))\n",
    "            elif norm == 'BN':\n",
    "                self.transform = nn.Sequential(\n",
    "                    nn.Linear(n_in, n_out, bias=False),\n",
    "                    nn.BatchNorm1d(n_out))\n",
    "            else:\n",
    "                exit('SyncBN has not been added!')\n",
    "        else:\n",
    "            self.transform = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.norm2(out)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            out += self.transform(x)\n",
    "        else:\n",
    "            out += x\n",
    "\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8451ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 2])\n"
     ]
    }
   ],
   "source": [
    "M=10\n",
    "K=6\n",
    "a = torch.randn([10, 1, 2])\n",
    "b = torch.randn([10, 6, 2])\n",
    "print((a-b).view(M*K, -1).shape)\n",
    "\n",
    "class AttDest(nn.Module):\n",
    "    def __init__(self, n_agt):\n",
    "        super(AttDest, self).__init__()\n",
    "        norm = \"GN\"\n",
    "        ng = 1\n",
    "        \n",
    "        # take position, get target-sized vector\n",
    "        self.dist = nn.Sequential(\n",
    "            nn.Linear(2, n_agt),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Linear(n_agt, n_agt, norm=norm, ng=ng),\n",
    "        )\n",
    "        self.agt = Linear(2 * n_agt, n_agt, norm=norm, ng=ng)\n",
    "        \n",
    "    def forward(self, agts: Tensor, agt_ctrs: Tensor, dest_ctrs: Tensor) -> Tensor:\n",
    "        'dest_ctrs: MxKx2?'\n",
    "        n_agt = agts.size(1)\n",
    "        num_mods = dest_ctrs.size(1)\n",
    "        \n",
    "        dist = (agt_ctrs.unsqueeze(1) - dest_ctrs).view(-1, 2) #\n",
    "        dist = self.dist(dist)\n",
    "        \n",
    "        agts = agts.unsqueeze(1).repeat(1, num_mods, 1).view(-1, n_agt)\n",
    "        \n",
    "        agts = torch.cat((dist, agts), 1)\n",
    "        agts = self.agt(agts) #[MxK, 2*agt_dim]\n",
    "        return agts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7818e253",
   "metadata": {},
   "source": [
    "## Prednet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f05ac8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): Sequential(\n",
      "    (0): LinearRes(\n",
      "      (linear1): Linear(in_features=128, out_features=128, bias=False)\n",
      "      (linear2): Linear(in_features=128, out_features=128, bias=False)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (norm1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "      (norm2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "    )\n",
      "    (1): Linear(in_features=128, out_features=60, bias=True)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): LinearRes(\n",
      "      (linear1): Linear(in_features=128, out_features=128, bias=False)\n",
      "      (linear2): Linear(in_features=128, out_features=128, bias=False)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (norm1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "      (norm2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "    )\n",
      "    (1): Linear(in_features=128, out_features=60, bias=True)\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): LinearRes(\n",
      "      (linear1): Linear(in_features=128, out_features=128, bias=False)\n",
      "      (linear2): Linear(in_features=128, out_features=128, bias=False)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (norm1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "      (norm2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "    )\n",
      "    (1): Linear(in_features=128, out_features=60, bias=True)\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): LinearRes(\n",
      "      (linear1): Linear(in_features=128, out_features=128, bias=False)\n",
      "      (linear2): Linear(in_features=128, out_features=128, bias=False)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (norm1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "      (norm2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "    )\n",
      "    (1): Linear(in_features=128, out_features=60, bias=True)\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    (0): LinearRes(\n",
      "      (linear1): Linear(in_features=128, out_features=128, bias=False)\n",
      "      (linear2): Linear(in_features=128, out_features=128, bias=False)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (norm1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "      (norm2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "    )\n",
      "    (1): Linear(in_features=128, out_features=60, bias=True)\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): LinearRes(\n",
      "      (linear1): Linear(in_features=128, out_features=128, bias=False)\n",
      "      (linear2): Linear(in_features=128, out_features=128, bias=False)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (norm1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "      (norm2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "    )\n",
      "    (1): Linear(in_features=128, out_features=60, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_548767/1496181244.py:36: DeprecationWarning: fractions.gcd() is deprecated. Use math.gcd() instead.\n",
      "  self.norm1 = nn.GroupNorm(gcd(ng, n_out), n_out)\n",
      "/tmp/ipykernel_548767/1496181244.py:37: DeprecationWarning: fractions.gcd() is deprecated. Use math.gcd() instead.\n",
      "  self.norm2 = nn.GroupNorm(gcd(ng, n_out), n_out)\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "n_actor = config[\"n_actor\"]\n",
    "norm = \"GN\"\n",
    "ng = 1\n",
    "\n",
    "for i in range(config[\"num_mods\"]):\n",
    "    pred.append(\n",
    "        nn.Sequential(\n",
    "            LinearRes(n_actor, n_actor, norm=norm, ng=ng),\n",
    "            nn.Linear(n_actor, 2 * config[\"num_preds\"]),\n",
    "        )\n",
    "    )\n",
    "print(nn.ModuleList(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fd2cc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Final motion forecasting with Linear Residual block\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(PredNet, self).__init__()\n",
    "        self.config = config\n",
    "        norm = \"GN\"\n",
    "        ng = 1\n",
    "        \n",
    "        n_actor = config[\"n_actor\"]\n",
    "        \n",
    "        pred = []\n",
    "        for i in range(config[\"num_mods\"]):\n",
    "            pred.append(\n",
    "                nn.Sequential(\n",
    "                    LinearRes(n_actor, n_actor, norm=norm, ng=ng),\n",
    "                    nn.Linear(n_actor, 2 * config[\"num_preds\"]),\n",
    "                )\n",
    "            )\n",
    "        self.pred = nn.ModuleList(pred)\n",
    "        \n",
    "        # return a concatenated and linearly transformed vector of [agt,dist_from_final_pos]: [MxK, 2*agt_dim]\n",
    "        self.att_dest = AttDest(n_actor)\n",
    "        \n",
    "        # classification\n",
    "        self.cls = nn.Sequential(\n",
    "            LinearRes(n_actor, n_actor, norm=norm, ng=ng), \n",
    "            nn.Linear(n_actor, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, actors: Tensor, actor_idcs: List[Tensor], actor_ctrs: List[Tensor]) -> Dict[str, List[Tensor]]:\n",
    "        preds = []\n",
    "        \n",
    "        for i in range(len(self.pred)): #pred:6\n",
    "            preds.append(self.pred[i](actors))\n",
    "        \n",
    "        reg = torch.cat([x.unsqueeze(1) for x in preds], 1) #MxKx60\n",
    "        reg = reg.view(reg.size(0), reg.size(1), -1, 2) #(MxKx30x2)\n",
    "        \n",
    "        for i in range(len(actor_idcs)):\n",
    "            # what is happening here? Add ctrs to reg to get final locations\n",
    "            idcs = actor_idcs[i]\n",
    "            ctrs = actor_ctrs[i].view(-1, 1, 1, 2)\n",
    "            reg[idcs] = reg[idcs] + ctrs\n",
    "            \n",
    "        # what is happening here?\n",
    "        # we have the regression - onto classification\n",
    "        dest_ctrs = reg[:, :, -1].detach()\n",
    "        feats = self.att_dest(actors, torch.cat(actor_ctrs, 0), dest_ctrs) # concatenate with end-points and transform to feed to classifier\n",
    "        # feats: [MxK, 2*agt_dim]; feed to scoring function\n",
    "        cls = self.cls(feats).view(-1, self.config[\"num_mods\"]) #(MxK,1)->(M, K)\n",
    "        \n",
    "        # Sorting time\n",
    "        cls, sort_idcs = cls.sort(1, descending=True)\n",
    "        row_idcs = torch.arange(len(sort_idcs)).long().to(sort_idcs.device)\n",
    "        row_idcs = row_idcs.view(-1, 1).repeat(1, sort_idcs.size(1)).view(-1)\n",
    "        sort_idcs = sort_idcs.view(-1)\n",
    "        reg = reg[row_idcs, sort_idcs].view(cls.size(0), cls.size(1), -1, 2) # sort the regression to be in order of confidence\n",
    "        \n",
    "        # time to ouput\n",
    "        out = dict()\n",
    "        out[\"cls\"], out[\"reg\"] = [], []\n",
    "        for i in range(len(actor_idcs)):\n",
    "            idcs = actor_idcs[i]\n",
    "            out[\"cls\"].append(cls[idcs])\n",
    "            out[\"reg\"].append(reg[idcs])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629992e7",
   "metadata": {},
   "source": [
    "## Convert to original coordinates as gt_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9df635a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(out[\"reg\"])):\n",
    "#    out[\"reg\"][i] = torch.matmul(out[\"reg\"][i], rot[i]) + orig[i].view(1, 1, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace01d55",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "346683ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) tensor([2, 2, 5, 1, 5, 3, 0, 1, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "dist = []\n",
    "reg = torch.randn([10, 6, 30, 2])\n",
    "gt_preds = torch.randn([10, 30, 2])\n",
    "row_idcs = torch.arange(10)\n",
    "\n",
    "for j in range(6):\n",
    "    dist.append(\n",
    "        torch.sqrt(((reg[row_idcs, j, -1] - gt_preds[row_idcs, -1])** 2).sum(1))\n",
    "    )\n",
    "dist = torch.cat([x.unsqueeze(1) for x in dist], 1)\n",
    "min_dist, min_idcs = dist.min(1)\n",
    "row_idcs = torch.arange(len(min_idcs)).long().to(min_idcs.device)\n",
    "print(row_idcs, min_idcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb33ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredLoss(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(PredLoss, self).__init__()\n",
    "        self.config = config\n",
    "        self.reg_loss = nn.SmoothL1Loss(reduction=\"sum\")\n",
    "        \n",
    "    def forward(self, out, gt_preds, has_preds):\n",
    "        \"\"\"\n",
    "        Loss = Margin loss + regression loss\n",
    "        Input:\n",
    "        out: Dict[str, List[Tensor]], \n",
    "        gt_preds: List[Tensor], \n",
    "        has_preds: List[Tensor])\n",
    "        Output:\n",
    "        loss_out: Dict[str, Union[Tensor, int]]\n",
    "        \"\"\"\n",
    "        cls, reg = out[\"cls\"], out[\"reg\"]\n",
    "        \n",
    "        #------from list to tensor\n",
    "        cls = torch.cat([x for x in cls], 0) # convert from list[tensor]-> tensor with size(0)=len(list[tensor])\n",
    "        reg = torch.cat([x for x in reg], 0)\n",
    "        \n",
    "        gt_preds = torch.cat([x for x in gt_preds], 0)\n",
    "        has_preds = torch.cat([x for x in has_preds], 0)\n",
    "        \n",
    "        #-------init\n",
    "        loss_out = dict()\n",
    "        zero = 0.0 * (cls.sum() + reg.sum()) #K?\n",
    "        loss_out[\"cls_loss\"] = zero.clone()\n",
    "        loss_out[\"num_cls\"] = 0\n",
    "        loss_out[\"reg_loss\"] = zero.clone()\n",
    "        loss_out[\"num_reg\"] = 0\n",
    "        \n",
    "        num_mods, num_preds = self.config[\"num_mods\"], self.config[\"num_preds\"]\n",
    "        # assert(has_preds.all())\n",
    "        \n",
    "        #-------masking based on has_preds\n",
    "        last = has_preds.float() + 0.1 * torch.arange(num_preds).float().to(\n",
    "            has_preds.device\n",
    "        ) / float(num_preds)\n",
    "        max_last, last_idcs = last.max(1)\n",
    "        mask = max_last > 1.0\n",
    "\n",
    "        cls = cls[mask]\n",
    "        reg = reg[mask]\n",
    "        gt_preds = gt_preds[mask]\n",
    "        has_preds = has_preds[mask]\n",
    "        last_idcs = last_idcs[mask]\n",
    "        \n",
    "        #-------finding what is k^ based on min_dist of mode to gt_pred\n",
    "        row_idcs = torch.arange(len(last_idcs)).long().to(last_idcs.device)\n",
    "        dist = []\n",
    "        for j in range(num_mods):\n",
    "            dist.append(torch.sqrt(((reg[row_idcs, j, last_idcs] - gt_preds[row_idcs, last_idcs])** 2).sum(1)))\n",
    "        dist = torch.cat([x.unsqueeze(1) for x in dist], 1)\n",
    "        # Returns a named tuple (values, indices) where values is the minimum value of each row of the input \n",
    "        # tensor in the given dimension dim. And indices is the index location of each minimum value found (argmin).\n",
    "        min_dist, min_idcs = dist.min(1)\n",
    "        row_idcs = torch.arange(len(min_idcs)).long().to(min_idcs.device)\n",
    "        \n",
    "        #-------classification loss (margin loss)\n",
    "        mgn = cls[row_idcs, min_idcs].unsqueeze(1) - cls\n",
    "        mask0 = (min_dist < self.config[\"cls_th\"]).view(-1, 1)\n",
    "        mask1 = dist - min_dist.view(-1, 1) > self.config[\"cls_ignore\"]\n",
    "        mgn = mgn[mask0 * mask1]\n",
    "        mask = mgn < self.config[\"mgn\"]\n",
    "        coef = self.config[\"cls_coef\"]\n",
    "        loss_out[\"cls_loss\"] += coef * (\n",
    "            self.config[\"mgn\"] * mask.sum() - mgn[mask].sum()\n",
    "        )\n",
    "        loss_out[\"num_cls\"] += mask.sum().item()\n",
    "        \n",
    "        #--------regression loss: easier\n",
    "        reg = reg[row_idcs, min_idcs]\n",
    "        coef = self.config[\"reg_coef\"]\n",
    "        loss_out[\"reg_loss\"] += coef * self.reg_loss(reg[has_preds], gt_preds[has_preds])\n",
    "        loss_out[\"num_reg\"] += has_preds.sum().item()\n",
    "        \n",
    "        return loss_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9625ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Loss, self).__init__()\n",
    "        self.config = config\n",
    "        self.pred_loss = PredLoss(config)\n",
    "\n",
    "    def forward(self, out: Dict, data: Dict) -> Dict:\n",
    "        loss_out = self.pred_loss(out, \n",
    "                                  gpu(data[\"gt_preds\"]), \n",
    "                                  gpu(data[\"has_preds\"]))\n",
    "        #cls_loss (divide by (num_traj-1)*num_actors) \n",
    "        #reg_loss (divide by num_actors*future_time_steps)\n",
    "        loss_out[\"loss\"] = loss_out[\"cls_loss\"] / (loss_out[\"num_cls\"] + 1e-10) + loss_out[\"reg_loss\"] / (loss_out[\"num_reg\"] + 1e-10)\n",
    "        return loss_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e52cf35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl_venv",
   "language": "python",
   "name": "ssl_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
